# Explainable AI (xAI) 
*List of notebooks focused on Explainable AI (xAI)*
***

## Verfication vs. validation
- Verification is telling you whether you solved the equation **right**.
- Validation is telling you whether you solved the **right** equation. 
- Rather than focusing on explanations, ML practioner should really concentrate on is performance and whether that performance has been tested in a rigorous, scientific manner. There is a nice parallel of this way of thinking: in medicine is full of drugs and techniques that doctors use because they work, even though no one knows why acetaminophen has been used for a century to treat pain and inflammation, even though we still don’t fully understand the underlying mechanism.
- In other words, what we should care about when it comes to A.I. in the real world is not explanation. It is validation.
***

## A note on the notebook rendering
Each notebook has two versions (all python scripts are unaffected by this):
- One where all the markdown comments are rendered in black& white. These are placed in the folder named `GitHub_MD_rendering` where MD stands for MarkDown.
- One where all the markdown comments are rendered in coloured.
***

## Available tutorials
- [Advanced uses of SHAP values]()
- [Common pitfalls in linear model coefficients interpretation]()
- [Drop_column_feature]()
- [Explain NLP Models with LIME]()
- [Introduction to permutation importance]()
- [PDF = Partial Dependence Plot & ICE = Individual Conditional Expectation]()
- [PDP = Partial Dependence Plots V2]()
- [Permuatation importance in random forest - cardinality]()
- [Permutation Importance with Collinearity]()
- [SHAP - SHapley Additive exPlanations]()
- [SHapley Additive exPlanation]()

## References
- [What’s wrong with “explainable A.I.”](https://fortune.com/2022/03/22/ai-explainable-radiology-medicine-crisis-eye-on-ai/)
- [How to build TRUST in Machine Learning, the sane way](https://medium.com/bigabids-dataverse/how-to-build-trust-in-machine-learning-the-sane-way-39d879f22e69)
***
